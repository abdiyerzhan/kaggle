{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa7655a-7944-413c-a2cb-9b20b6692f5f",
   "metadata": {},
   "source": [
    "## Задание: Реализация полносвязной нейронной сети\n",
    "### Цель\n",
    "#### Реализовать простую полносвязную нейронную сеть с нуля, используя только NumPy, и применить ее для решения задачи классификации.\n",
    "#### Описание задания\n",
    "#### Реализуйте класс NeuralNetwork со следующей архитектурой:\n",
    "Входной слой (размер зависит от данных)\n",
    "Один скрытый слой (64 нейрона)\n",
    "Выходной слой (размер зависит от количества классов)\n",
    "Реализуйте следующие методы:\n",
    "__init__: инициализация весов и смещений\n",
    "forward: прямое распространение\n",
    "backward: обратное распространение\n",
    "train: обучение сети\n",
    "predict: предсказание на новых данных\n",
    "Используйте следующие функции активации:\n",
    "ReLU для скрытого слоя\n",
    "Softmax для выходного слоя\n",
    "Используйте перекрестную энтропию в качестве функции потерь.\n",
    "Протестируйте вашу сеть на наборе данных MNIST (рукописные цифры).\n",
    "Требования к реализации\n",
    "Используйте только NumPy для вычислений.\n",
    "Реализуйте мини-пакетный градиентный спуск.\n",
    "Добавьте возможность настройки гиперпараметров (скорость обучения, размер мини-пакета, количество эпох).\n",
    "Дополнительные задания (по желанию)\n",
    "Добавьте регуляризацию L2.\n",
    "Реализуйте dropout.\n",
    "Добавьте возможность использовать произвольное количество скрытых слоев.\n",
    "Оценка\n",
    "Корректность реализации: 50%\n",
    "Качество кода и документации: 20%\n",
    "Производительность на тестовом наборе MNIST: 20%\n",
    "Дополнительные задания: 10%\n",
    "Загрузка на GitHub\n",
    "Создайте репозиторий на GitHub.\n",
    "Загрузите все файлы проекта, включая код, отчет и визуализации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b1faf4-6dc4-479b-b29b-86f0c3b334ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f56403-f781-4607-842f-c63fd69efc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Инициализация весов и смещений\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, labels):\n",
    "        n_samples = labels.shape[0]\n",
    "        log_likelihood = -np.log(predictions[range(n_samples), labels])\n",
    "        return np.mean(log_likelihood)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.relu(self.hidden_input)\n",
    "\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        output = self.softmax(self.output_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        n_samples = X.shape[0]\n",
    "        y_one_hot = np.zeros_like(output)\n",
    "        y_one_hot[np.arange(n_samples), y] = 1\n",
    "\n",
    "        d_output_input = (output - y_one_hot) / n_samples\n",
    "        d_weights_hidden_output = np.dot(self.hidden_output.T, d_output_input)\n",
    "        d_bias_output = np.sum(d_output_input, axis=0, keepdims=True)\n",
    "\n",
    "        d_hidden_output = np.dot(d_output_input, self.weights_hidden_output.T) * self.relu_derivative(self.hidden_input)\n",
    "        d_weights_input_hidden = np.dot(X.T, d_hidden_output)\n",
    "        d_bias_hidden = np.sum(d_hidden_output, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights_input_hidden -= self.learning_rate * d_weights_input_hidden\n",
    "        self.bias_hidden -= self.learning_rate * d_bias_hidden\n",
    "        self.weights_hidden_output -= self.learning_rate * d_weights_hidden_output\n",
    "        self.bias_output -= self.learning_rate * d_bias_output\n",
    "\n",
    "    def train(self, X, y, epochs=10, batch_size=32):\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "\n",
    "                output = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch, output)\n",
    "\n",
    "            predictions = self.predict(X)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b96224-6e5a-43bc-a8af-92a6918f4375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Accuracy: 0.8471\n",
      "Epoch 2/150, Accuracy: 0.8916\n",
      "Epoch 3/150, Accuracy: 0.9039\n",
      "Epoch 4/150, Accuracy: 0.9115\n",
      "Epoch 5/150, Accuracy: 0.9170\n",
      "Epoch 6/150, Accuracy: 0.9215\n",
      "Epoch 7/150, Accuracy: 0.9253\n",
      "Epoch 8/150, Accuracy: 0.9296\n",
      "Epoch 9/150, Accuracy: 0.9339\n",
      "Epoch 10/150, Accuracy: 0.9376\n",
      "Epoch 11/150, Accuracy: 0.9411\n",
      "Epoch 12/150, Accuracy: 0.9443\n",
      "Epoch 13/150, Accuracy: 0.9471\n",
      "Epoch 14/150, Accuracy: 0.9493\n",
      "Epoch 15/150, Accuracy: 0.9516\n",
      "Epoch 16/150, Accuracy: 0.9533\n",
      "Epoch 17/150, Accuracy: 0.9549\n",
      "Epoch 18/150, Accuracy: 0.9569\n",
      "Epoch 19/150, Accuracy: 0.9586\n",
      "Epoch 20/150, Accuracy: 0.9599\n",
      "Epoch 21/150, Accuracy: 0.9614\n",
      "Epoch 22/150, Accuracy: 0.9629\n",
      "Epoch 23/150, Accuracy: 0.9641\n",
      "Epoch 24/150, Accuracy: 0.9654\n",
      "Epoch 25/150, Accuracy: 0.9663\n",
      "Epoch 26/150, Accuracy: 0.9674\n",
      "Epoch 27/150, Accuracy: 0.9688\n",
      "Epoch 28/150, Accuracy: 0.9696\n",
      "Epoch 29/150, Accuracy: 0.9703\n",
      "Epoch 30/150, Accuracy: 0.9711\n",
      "Epoch 31/150, Accuracy: 0.9720\n",
      "Epoch 32/150, Accuracy: 0.9728\n",
      "Epoch 33/150, Accuracy: 0.9732\n",
      "Epoch 34/150, Accuracy: 0.9739\n",
      "Epoch 35/150, Accuracy: 0.9746\n",
      "Epoch 36/150, Accuracy: 0.9754\n",
      "Epoch 37/150, Accuracy: 0.9761\n",
      "Epoch 38/150, Accuracy: 0.9767\n",
      "Epoch 39/150, Accuracy: 0.9771\n",
      "Epoch 40/150, Accuracy: 0.9774\n",
      "Epoch 41/150, Accuracy: 0.9778\n",
      "Epoch 42/150, Accuracy: 0.9784\n",
      "Epoch 43/150, Accuracy: 0.9786\n",
      "Epoch 44/150, Accuracy: 0.9791\n",
      "Epoch 45/150, Accuracy: 0.9794\n",
      "Epoch 46/150, Accuracy: 0.9801\n",
      "Epoch 47/150, Accuracy: 0.9806\n",
      "Epoch 48/150, Accuracy: 0.9811\n",
      "Epoch 49/150, Accuracy: 0.9814\n",
      "Epoch 50/150, Accuracy: 0.9818\n",
      "Epoch 51/150, Accuracy: 0.9821\n",
      "Epoch 52/150, Accuracy: 0.9824\n",
      "Epoch 53/150, Accuracy: 0.9827\n",
      "Epoch 54/150, Accuracy: 0.9830\n",
      "Epoch 55/150, Accuracy: 0.9834\n",
      "Epoch 56/150, Accuracy: 0.9838\n",
      "Epoch 57/150, Accuracy: 0.9841\n",
      "Epoch 58/150, Accuracy: 0.9844\n",
      "Epoch 59/150, Accuracy: 0.9848\n",
      "Epoch 60/150, Accuracy: 0.9851\n",
      "Epoch 61/150, Accuracy: 0.9853\n",
      "Epoch 62/150, Accuracy: 0.9856\n",
      "Epoch 63/150, Accuracy: 0.9859\n",
      "Epoch 64/150, Accuracy: 0.9861\n",
      "Epoch 65/150, Accuracy: 0.9862\n",
      "Epoch 66/150, Accuracy: 0.9865\n",
      "Epoch 67/150, Accuracy: 0.9867\n",
      "Epoch 68/150, Accuracy: 0.9868\n",
      "Epoch 69/150, Accuracy: 0.9870\n",
      "Epoch 70/150, Accuracy: 0.9873\n",
      "Epoch 71/150, Accuracy: 0.9875\n",
      "Epoch 72/150, Accuracy: 0.9877\n",
      "Epoch 73/150, Accuracy: 0.9879\n",
      "Epoch 74/150, Accuracy: 0.9881\n",
      "Epoch 75/150, Accuracy: 0.9885\n",
      "Epoch 76/150, Accuracy: 0.9887\n",
      "Epoch 77/150, Accuracy: 0.9889\n",
      "Epoch 78/150, Accuracy: 0.9891\n",
      "Epoch 79/150, Accuracy: 0.9893\n",
      "Epoch 80/150, Accuracy: 0.9894\n",
      "Epoch 81/150, Accuracy: 0.9896\n",
      "Epoch 82/150, Accuracy: 0.9897\n",
      "Epoch 83/150, Accuracy: 0.9899\n",
      "Epoch 84/150, Accuracy: 0.9900\n",
      "Epoch 85/150, Accuracy: 0.9902\n",
      "Epoch 86/150, Accuracy: 0.9904\n",
      "Epoch 87/150, Accuracy: 0.9905\n",
      "Epoch 88/150, Accuracy: 0.9906\n",
      "Epoch 89/150, Accuracy: 0.9908\n",
      "Epoch 90/150, Accuracy: 0.9911\n",
      "Epoch 91/150, Accuracy: 0.9913\n",
      "Epoch 92/150, Accuracy: 0.9915\n",
      "Epoch 93/150, Accuracy: 0.9915\n",
      "Epoch 94/150, Accuracy: 0.9917\n",
      "Epoch 95/150, Accuracy: 0.9919\n",
      "Epoch 96/150, Accuracy: 0.9919\n",
      "Epoch 97/150, Accuracy: 0.9921\n",
      "Epoch 98/150, Accuracy: 0.9923\n",
      "Epoch 99/150, Accuracy: 0.9925\n",
      "Epoch 100/150, Accuracy: 0.9926\n",
      "Epoch 101/150, Accuracy: 0.9928\n",
      "Epoch 102/150, Accuracy: 0.9929\n",
      "Epoch 103/150, Accuracy: 0.9931\n",
      "Epoch 104/150, Accuracy: 0.9931\n",
      "Epoch 105/150, Accuracy: 0.9932\n",
      "Epoch 106/150, Accuracy: 0.9933\n",
      "Epoch 107/150, Accuracy: 0.9934\n",
      "Epoch 108/150, Accuracy: 0.9936\n",
      "Epoch 109/150, Accuracy: 0.9936\n",
      "Epoch 110/150, Accuracy: 0.9938\n",
      "Epoch 111/150, Accuracy: 0.9939\n",
      "Epoch 112/150, Accuracy: 0.9940\n",
      "Epoch 113/150, Accuracy: 0.9941\n",
      "Epoch 114/150, Accuracy: 0.9942\n",
      "Epoch 115/150, Accuracy: 0.9944\n",
      "Epoch 116/150, Accuracy: 0.9945\n",
      "Epoch 117/150, Accuracy: 0.9946\n",
      "Epoch 118/150, Accuracy: 0.9948\n",
      "Epoch 119/150, Accuracy: 0.9948\n",
      "Epoch 120/150, Accuracy: 0.9949\n",
      "Epoch 121/150, Accuracy: 0.9950\n",
      "Epoch 122/150, Accuracy: 0.9950\n",
      "Epoch 123/150, Accuracy: 0.9951\n",
      "Epoch 124/150, Accuracy: 0.9952\n",
      "Epoch 125/150, Accuracy: 0.9953\n",
      "Epoch 126/150, Accuracy: 0.9954\n",
      "Epoch 127/150, Accuracy: 0.9954\n",
      "Epoch 128/150, Accuracy: 0.9955\n",
      "Epoch 129/150, Accuracy: 0.9955\n",
      "Epoch 130/150, Accuracy: 0.9956\n",
      "Epoch 131/150, Accuracy: 0.9957\n",
      "Epoch 132/150, Accuracy: 0.9957\n",
      "Epoch 133/150, Accuracy: 0.9958\n",
      "Epoch 134/150, Accuracy: 0.9959\n",
      "Epoch 135/150, Accuracy: 0.9960\n",
      "Epoch 136/150, Accuracy: 0.9961\n",
      "Epoch 137/150, Accuracy: 0.9961\n",
      "Epoch 138/150, Accuracy: 0.9963\n",
      "Epoch 139/150, Accuracy: 0.9963\n",
      "Epoch 140/150, Accuracy: 0.9964\n",
      "Epoch 141/150, Accuracy: 0.9965\n",
      "Epoch 142/150, Accuracy: 0.9966\n",
      "Epoch 143/150, Accuracy: 0.9966\n",
      "Epoch 144/150, Accuracy: 0.9967\n",
      "Epoch 145/150, Accuracy: 0.9968\n",
      "Epoch 146/150, Accuracy: 0.9969\n",
      "Epoch 147/150, Accuracy: 0.9969\n",
      "Epoch 148/150, Accuracy: 0.9970\n",
      "Epoch 149/150, Accuracy: 0.9971\n",
      "Epoch 150/150, Accuracy: 0.9971\n",
      "Test Accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Загрузка и подготовка данных MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "\n",
    "# Нормализация данных и разбиение на обучающую и тестовую выборки\n",
    "X = X / 255.0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Параметры сети\n",
    "input_size = 784  # 28x28 пикселей\n",
    "hidden_size = 64\n",
    "output_size = 10  # Классы от 0 до 9\n",
    "\n",
    "# Создание и обучение модели\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate=0.01)\n",
    "nn.train(X_train, y_train, epochs=150, batch_size=32)\n",
    "\n",
    "# Тестирование модели\n",
    "y_pred = nn.predict(X_test)\n",
    "test_accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c416c9-262c-425a-b66f-8eb9dceb32a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a44890-0fbc-4350-9780-415f47d7a8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd587ac-91a3-4a1a-a9ae-9edd76c7b652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3838f84-0bc4-4c59-be61-63c53001b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b12658-bdb5-4db9-86a1-ff4213ffc092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, l2_lambda=0.0, dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        layer_sizes: список, где первый элемент - размер входного слоя, \n",
    "                     последний элемент - размер выходного слоя, а промежуточные - размеры скрытых слоев\n",
    "        learning_rate: скорость обучения\n",
    "        l2_lambda: коэффициент регуляризации L2 (если 0, регуляризация отключена)\n",
    "        dropout_rate: доля отключаемых нейронов для Dropout (если 0, Dropout отключен)\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Инициализация весов и смещений\n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01 for i in range(self.num_layers - 1)]\n",
    "        self.biases = [np.zeros((1, layer_sizes[i+1])) for i in range(self.num_layers - 1)]\n",
    "        \n",
    "        # Маски для Dropout\n",
    "        self.dropout_masks = [None] * (self.num_layers - 1)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, predictions, labels):\n",
    "        n_samples = labels.shape[0]\n",
    "        log_likelihood = -np.log(predictions[range(n_samples), labels])\n",
    "        return np.mean(log_likelihood)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        activations = [X]\n",
    "        \n",
    "        for i in range(self.num_layers - 2):\n",
    "            # Прямое распространение на скрытых слоях с ReLU\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self.relu(z)\n",
    "            \n",
    "            # Применение Dropout (только на этапе обучения)\n",
    "            if training and self.dropout_rate > 0:\n",
    "                self.dropout_masks[i] = (np.random.rand(*a.shape) > self.dropout_rate).astype(float) / (1.0 - self.dropout_rate)\n",
    "                a *= self.dropout_masks[i]\n",
    "                \n",
    "            activations.append(a)\n",
    "\n",
    "        # Прямое распространение на выходном слое с Softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        a = self.softmax(z)\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def backward(self, activations, y):\n",
    "        n_samples = y.shape[0]\n",
    "        y_one_hot = np.zeros_like(activations[-1])\n",
    "        y_one_hot[np.arange(n_samples), y] = 1\n",
    "\n",
    "        # Градиент для выходного слоя\n",
    "        delta = (activations[-1] - y_one_hot) / n_samples\n",
    "        \n",
    "        for i in range(self.num_layers - 2, -1, -1):\n",
    "            # Градиенты для весов и смещений\n",
    "            d_weights = np.dot(activations[i].T, delta) + self.l2_lambda * self.weights[i]\n",
    "            d_biases = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "            # Обновление весов и смещений\n",
    "            self.weights[i] -= self.learning_rate * d_weights\n",
    "            self.biases[i] -= self.learning_rate * d_biases\n",
    "\n",
    "            if i > 0:  # Не вычисляем дельту для входного слоя\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(activations[i])\n",
    "\n",
    "                # Применение Dropout во время обратного распространения\n",
    "                if self.dropout_rate > 0:\n",
    "                    delta *= self.dropout_masks[i - 1]\n",
    "\n",
    "    def train(self, X, y, epochs=10, batch_size=32):\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "\n",
    "                activations = self.forward(X_batch)\n",
    "                self.backward(activations, y_batch)\n",
    "\n",
    "            predictions = self.predict(X)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = self.forward(X, training=False)\n",
    "        return np.argmax(activations[-1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fa241-06d8-4ed9-8bdb-d2d88785c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и подготовка данных MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d168076-562b-4631-96c5-a5802e5940d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0           0       0       0       0       0       0       0       0       0   \n",
       "1           0       0       0       0       0       0       0       0       0   \n",
       "2           0       0       0       0       0       0       0       0       0   \n",
       "3           0       0       0       0       0       0       0       0       0   \n",
       "4           0       0       0       0       0       0       0       0       0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "69995       0       0       0       0       0       0       0       0       0   \n",
       "69996       0       0       0       0       0       0       0       0       0   \n",
       "69997       0       0       0       0       0       0       0       0       0   \n",
       "69998       0       0       0       0       0       0       0       0       0   \n",
       "69999       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0            0  ...         0         0         0         0         0   \n",
       "1            0  ...         0         0         0         0         0   \n",
       "2            0  ...         0         0         0         0         0   \n",
       "3            0  ...         0         0         0         0         0   \n",
       "4            0  ...         0         0         0         0         0   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "69995        0  ...         0         0         0         0         0   \n",
       "69996        0  ...         0         0         0         0         0   \n",
       "69997        0  ...         0         0         0         0         0   \n",
       "69998        0  ...         0         0         0         0         0   \n",
       "69999        0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "69995         0         0         0         0         0  \n",
       "69996         0         0         0         0         0  \n",
       "69997         0         0         0         0         0  \n",
       "69998         0         0         0         0         0  \n",
       "69999         0         0         0         0         0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ee9fde-1978-4074-9c52-75f680ffb4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Accuracy: 0.1121\n",
      "Epoch 2/50, Accuracy: 0.1121\n",
      "Epoch 3/50, Accuracy: 0.1121\n",
      "Epoch 4/50, Accuracy: 0.2077\n",
      "Epoch 5/50, Accuracy: 0.3996\n",
      "Epoch 6/50, Accuracy: 0.5873\n",
      "Epoch 7/50, Accuracy: 0.6747\n",
      "Epoch 8/50, Accuracy: 0.7580\n",
      "Epoch 9/50, Accuracy: 0.8265\n",
      "Epoch 10/50, Accuracy: 0.8503\n",
      "Epoch 11/50, Accuracy: 0.8632\n",
      "Epoch 12/50, Accuracy: 0.8735\n",
      "Epoch 13/50, Accuracy: 0.8799\n",
      "Epoch 14/50, Accuracy: 0.8848\n",
      "Epoch 15/50, Accuracy: 0.8885\n",
      "Epoch 16/50, Accuracy: 0.8914\n",
      "Epoch 17/50, Accuracy: 0.8946\n",
      "Epoch 18/50, Accuracy: 0.8956\n",
      "Epoch 19/50, Accuracy: 0.8970\n",
      "Epoch 20/50, Accuracy: 0.8991\n",
      "Epoch 21/50, Accuracy: 0.8997\n",
      "Epoch 22/50, Accuracy: 0.9027\n",
      "Epoch 23/50, Accuracy: 0.9022\n",
      "Epoch 24/50, Accuracy: 0.9050\n",
      "Epoch 25/50, Accuracy: 0.9052\n",
      "Epoch 26/50, Accuracy: 0.9053\n",
      "Epoch 27/50, Accuracy: 0.9061\n",
      "Epoch 28/50, Accuracy: 0.9065\n",
      "Epoch 29/50, Accuracy: 0.9089\n",
      "Epoch 30/50, Accuracy: 0.9085\n",
      "Epoch 31/50, Accuracy: 0.9086\n",
      "Epoch 32/50, Accuracy: 0.9102\n",
      "Epoch 33/50, Accuracy: 0.9091\n",
      "Epoch 34/50, Accuracy: 0.9107\n",
      "Epoch 35/50, Accuracy: 0.9089\n",
      "Epoch 36/50, Accuracy: 0.9107\n",
      "Epoch 37/50, Accuracy: 0.9113\n",
      "Epoch 38/50, Accuracy: 0.9103\n",
      "Epoch 39/50, Accuracy: 0.9118\n",
      "Epoch 40/50, Accuracy: 0.9128\n",
      "Epoch 41/50, Accuracy: 0.9115\n",
      "Epoch 42/50, Accuracy: 0.9122\n",
      "Epoch 43/50, Accuracy: 0.9119\n",
      "Epoch 44/50, Accuracy: 0.9114\n",
      "Epoch 45/50, Accuracy: 0.9123\n",
      "Epoch 46/50, Accuracy: 0.9130\n",
      "Epoch 47/50, Accuracy: 0.9128\n",
      "Epoch 48/50, Accuracy: 0.9137\n",
      "Epoch 49/50, Accuracy: 0.9129\n",
      "Epoch 50/50, Accuracy: 0.9126\n",
      "Test Accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Нормализация данных и разбиение на обучающую и тестовую выборки\n",
    "X = X / 255.0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Параметры сети\n",
    "input_size = 784  # 28x28 пикселей\n",
    "hidden_layers = [64, 32]  # Произвольное количество скрытых слоев\n",
    "output_size = 10  # Классы от 0 до 9\n",
    "layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "# Создание и обучение модели\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=0.01, l2_lambda=0.01, dropout_rate=0.5)\n",
    "nn.train(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Тестирование модели\n",
    "y_pred = nn.predict(X_test)\n",
    "test_accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2346a3-1f75-4494-8a42-0e7c3ab4a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b\n",
      "0  1  16\n",
      "1  4  25\n",
      "2  9  36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.extensions import register_dataframe_accessor\n",
    "\n",
    "# Определим кастомный DataFrameAccessor\n",
    "@register_dataframe_accessor(\"custom\")\n",
    "class CustomMethods:\n",
    "    def __init__(self, df):\n",
    "        self._df = df\n",
    "\n",
    "    def square_columns(self):\n",
    "        return self._df ** 2\n",
    "\n",
    "# Теперь у любого DataFrame можно вызвать метод `custom.square_columns`\n",
    "df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "print(df.custom.square_columns())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb32d0b-61f0-4c56-8b45-82807883bb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b\n",
       "0  1  16\n",
       "1  4  25\n",
       "2  9  36"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.custom.square_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5254650-a6fe-472a-9d9d-d432497dc04a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
